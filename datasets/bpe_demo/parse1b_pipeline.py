import logging
from io import open
from conllu import parse_incr
import sentencepiece as spm


def conllu_to_text(input_file_path, output_file_path):
    """ reads input_file_path conllu file and saves raw text sentences to output_file_path
    """
    conllu_file = open(input_file_path, "r", encoding="utf-8")
    text_file = open(output_file_path, "w", encoding="utf-8")
    for tokenlist in parse_incr(conllu_file):
        text_file.write(' '.join([a['form'] for a in tokenlist]))
        text_file.write('\n')


def reconstruct_dependency_trees(conll_file_path, model_prefix, output_file_path):
    """ adds extra conll file for tokens that have been splitted by sentencepiece
    """
    conll_file = open(conll_file_path, "r", encoding="utf-8")
    output_file = open(output_file_path, "w", encoding="utf-8")

    conll_sentences = parse_incr(conll_file)

    sp = spm.SentencePieceProcessor()                                                                                                                                                                            
    sp.Load('%s.model' % model_prefix)

    for tokenlist in conll_sentences:
        sentence = ' '.join([a['form'] for a in tokenlist.tokens])
        pieces = sp.EncodeAsPieces(sentence)
        
        i_token = 0
        for i_pieces, piece in enumerate(pieces):

            piece = piece.replace('▁', '')
            import ipdb; ipdb.set_trace()

            token = tokenlist[i_token]
            if token['form'] != piece:  # the word has been splitted
                import ipdb; ipdb.set_trace()
            else:
                import ipdb; ipdb.set_trace()


if __name__ == "__main__":
    """ 
    """

    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s:\t%(message)s")

    # 0. Generate txt version of PTB

    if False:
        conllu_to_text('/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/dev.gold.conll', '/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/dev.gold.txt')
        conllu_to_text('/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/test.gold.conll', '/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/test.gold.txt')
        conllu_to_text('/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/train.gold.conll', '/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/train.gold.txt')


    # 1. train bpe model with [raw 1b text training set + raw penn training set] 
    #    as input, 80k vocab --> bpe.model; bpe.vocab

    input_files = '/home/lpmayos/code/UniParse/datasets/PTB_SD_3_3_0/train.gold.txt,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00000-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00001-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00002-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00003-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00004-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00005-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00006-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00007-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00008-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00009-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00010-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00011-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00012-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00013-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00014-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00015-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00016-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00017-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00018-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00019-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00020-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00021-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00022-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00023-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00024-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00025-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00026-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00027-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00028-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00029-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00030-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00031-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00032-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00033-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00034-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00035-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00036-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00037-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00038-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00039-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00040-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00041-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00042-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00043-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00044-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00045-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00046-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00047-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00048-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00049-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00050-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00051-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00052-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00053-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00054-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00055-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00056-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00057-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00058-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00059-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00060-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00061-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00062-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00063-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00064-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00065-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00066-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00067-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00068-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00069-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00070-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00071-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00072-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00073-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00074-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00075-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00076-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00077-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00078-of-00100,/home/lpmayos/code/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00079-of-00100'
    input_files_hpc = '/homedtic/lperez/UniParse/datasets/PTB_SD_3_3_0/train.gold.txt,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00001-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00002-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00003-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00004-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00005-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00006-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00007-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00008-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00009-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00010-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00011-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00012-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00013-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00014-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00015-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00016-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00017-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00018-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00019-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00020-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00021-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00022-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00023-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00024-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00025-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00026-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00027-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00028-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00029-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00030-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00031-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00032-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00033-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00034-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00035-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00036-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00037-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00038-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00039-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00040-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00041-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00042-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00043-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00044-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00045-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00046-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00047-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00048-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00049-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00050-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00051-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00052-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00053-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00054-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00055-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00056-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00057-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00058-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00059-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00060-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00061-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00062-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00063-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00064-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00065-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00066-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00067-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00068-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00069-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00070-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00071-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00072-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00073-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00074-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00075-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00076-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00077-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00078-of-00100,/homedtic/lperez/datasets/1-billion-word-language-modeling-benchmark-r13output/text/training-monolingual.tokenized.shuffled/news.en-00079-of-00100'
    vocab_size = '80000'
    model_type = 'bpe'
    model_prefix = 'm'
    character_coverage = '1.0'

    if True:
        spm.SentencePieceTrainer.Train('--input=%s --model_prefix=%s --vocab_size=%s  --character_coverage=%s --model_type=%s' % (input_files_hpc, model_prefix, vocab_size, character_coverage, model_type))


    # 2. encode penn (all files) using bpe.model (bash)

    # spm_encode --model=<model_file> --output_format=piece < input > output


    # 3. reconstruct penn dependency trees: iterate simultaneously over penn 
    #    enocoded files and conll files, splitting lines when necessary (if 
    #    a word is divided in two, first part is head and rest is dependant).
    #    --> penn_train_pieced.conll, penn_dev_pieced.conll and 
    #        penn_test_pieced.conll

    if False:
        conll_file = 'babau4.conllu'
        output_file = 'babau4_pieced.conllu'
        reconstruct_dependency_trees(conll_file, model_prefix, output_file)


    # 4. train kiperwasser/StanfordCoreNLP parser with penn_train_pieced.conll, 
    #    penn_dev_pieced.conll and penn_test_pieced.conll --> model1_pieced

    # 5. use kiperwasser/StanfordCoreNLP + model1_pieced to parse 
    #    1B input_pieced.txt[1b] --> 1b_train_pieced.conll, 1b_dev_pieced.conll, 
    #    1b_test_pieced.conll

    # 6. train kiperwasser parser with 1b_train_pieced.conll, 
    #    1b_dev_pieced.conll, 1b_test_pieced --> model3